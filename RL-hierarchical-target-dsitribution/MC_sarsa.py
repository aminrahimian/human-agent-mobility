import numpy as npfrom scipy.stats import normimport itertoolsfrom scipy.spatial import distanceimport copyimport pickleimport matplotlib.pyplot as pltimport time# generate targets.class Enviroment:    def __init__(self, L, n_cluster,corr,n_target_1, n_target_2=0,n_target_3=0):        self.L = L        target_global_sd_x=10        target_global_sd_y=10        target_cluster_sd_x = 4        target_cluster_sd_y = 4        self.n_threats=5        target_global_mean_x = 0.5 * L        target_global_mean_y = 0.5 * L        if n_cluster==1:            center_target_1_x = norm.rvs(loc=target_global_mean_x, scale=target_global_sd_x, size=1)[0]            center_target_1_y = norm.rvs(loc=target_global_mean_y, scale=target_global_sd_y, size=1)[0]            target_1_x = norm.rvs(loc=center_target_1_x, scale=target_cluster_sd_x, size=n_target_1)            target_1_y = norm.rvs(loc=center_target_1_y, scale=target_cluster_sd_y, size=n_target_1)            target_2_x = norm.rvs(loc=0, scale=0, size=0)            target_2_y = norm.rvs(loc=0, scale=0, size=0)            target_3_x = norm.rvs(loc=0, scale=0, size=0)            target_3_y = norm.rvs(loc=0, scale=0, size=0)            self.n_targets = n_target_1 + n_target_2 + n_target_3            target_x = np.concatenate((target_1_x, target_2_x, target_3_x))            target_y = np.concatenate((target_1_y, target_2_y, target_3_y))        elif n_cluster==2:            center_target_1_x = norm.rvs(loc=target_global_mean_x, scale=target_global_sd_x, size=n_cluster)            center_target_1_y = norm.rvs(loc=target_global_mean_y, scale=target_global_sd_y, size=n_cluster)            target_1_x = norm.rvs(loc=center_target_1_x[0], scale=target_cluster_sd_x, size=n_target_1)            target_1_y = norm.rvs(loc=center_target_1_y[0], scale=target_cluster_sd_y, size=n_target_1)            target_2_x = norm.rvs(loc=center_target_1_x[1], scale=target_cluster_sd_x, size=n_target_2)            target_2_y = norm.rvs(loc=center_target_1_y[1], scale=target_cluster_sd_y, size=n_target_2)            target_3_x = norm.rvs(loc=0, scale=0, size=0)            target_3_y = norm.rvs(loc=0, scale=0, size=0)            self.n_targets = n_target_1 + n_target_2 + n_target_3            target_x = np.concatenate((target_1_x, target_2_x, target_3_x))            target_y = np.concatenate((target_1_y, target_2_y, target_3_y))        else:            center_target_1_x = norm.rvs(loc=target_global_mean_x, scale=target_global_sd_x, size=n_cluster)            center_target_1_y = norm.rvs(loc=target_global_mean_y, scale=target_global_sd_y, size=n_cluster)            target_1_x = norm.rvs(loc=center_target_1_x[0], scale=target_cluster_sd_x, size=n_target_1)            target_1_y = norm.rvs(loc=center_target_1_y[0], scale=target_cluster_sd_y, size=n_target_1)            target_2_x = norm.rvs(loc=center_target_1_x[1], scale=target_cluster_sd_x, size=n_target_2)            target_2_y = norm.rvs(loc=center_target_1_y[1], scale=target_cluster_sd_y, size=n_target_2)            target_3_x = norm.rvs(loc=center_target_1_x[2], scale=target_cluster_sd_x, size=n_target_3)            target_3_y = norm.rvs(loc=center_target_1_y[2], scale=target_cluster_sd_y, size=n_target_3)            self.n_targets = n_target_1 + n_target_2 + n_target_3            target_x = np.concatenate((target_1_x, target_2_x, target_3_x))            target_y = np.concatenate((target_1_y, target_2_y, target_3_y))        keys_dict_targets=list(range(self.n_targets))        coordinates_targets = [(target_x[i], target_y[i],0) for i in range(self.n_targets)]        # the first element is s, second a        self.targets = dict(zip(keys_dict_targets, coordinates_targets))        mean = np.array([target_global_mean_x, target_global_mean_x])        sigmas = np.array([[target_global_sd_x, 0], [0, target_global_sd_x]])        pearson_corr= np.array([[1, corr], [corr, 1]])        cov=sigmas@pearson_corr@sigmas        _, x_threats = np.random.multivariate_normal(mean, cov, self.n_threats).T        _, y_threats = np.random.multivariate_normal(mean, cov, self.n_threats).T        keys_dict_threats = list(range(self.n_threats))        coordinates_threats = [(x_threats[i], y_threats[i], 0) for i in range(self.n_threats)]        self.threats= dict(zip(keys_dict_threats, coordinates_threats))    def update_target_status(self, key_dict):        new_tuple = (self.targets[key_dict][0],self.targets[key_dict][1],1)        self.targets[key_dict] = new_tuple    def update_threat_status(self, key_dict):        new_tuple = (self.threats[key_dict][0], self.threats[key_dict][1], 1)        self.threats[key_dict] = new_tuple    def reset_target_status(self):        for t in range(self.n_targets):            self.targets[t]=(self.targets[t][0],self.targets[t][1],0)        for t in range(self.n_threats):            self.threats[t] = (self.threats[t][0], self.threats[t][1], 0)class Agent:    def __init__(self, w_cell, L, T, velocities, max_targets, freq_sampling,                 radius,episodes):        self.L = L        self.T = T        n_x = int(np.floor(L / w_cell))        n_y = int(np.floor(L / w_cell))        self.max_targets = max_targets        self.n_threats=5        self.w = w_cell        self.n_x=n_x        self.n_y=n_y        self.grid = max_targets*np.ones((n_x, n_y, len(velocities)))        self.C=np.zeros((n_x, n_y, len(velocities)))        self.counts=np.zeros((n_x, n_y, len(velocities)))        self.policy = np.zeros((n_x, n_y))        self.pos_x = 0        self.pos_y = 0        self.velocities = velocities        self.freq_sampling = freq_sampling  # times per alg time step        self.radius = radius        self.cum_targets = 0        self.alpha = 0.9        self.gamma = 0.01        self.time_step = 0        self.actions = [0] * T        self.episodes=episodes        self.paths_x=np.zeros((episodes,self.T))        self.paths_y=np.zeros((episodes,self.T))        self.targets_x=np.zeros((episodes,self.max_targets))        self.targets_y=np.zeros((episodes,self.max_targets))        self.threats_x=np.zeros((episodes,self.n_threats))        self.threats_y=np.zeros((episodes,self.n_threats))        grid_line = 1000 * np.arange(0.5, 10.5, 1)        grid_space = itertools.product(grid_line, grid_line)        center_lay1 = list(itertools.product(grid_space, grid_space))        center_lay2 = list(itertools.product(grid_space, grid_space))        center_init=[(5000,5000)]        self.centers=center_lay1+center_lay2+center_init        self.weigths_vector=[0]+len(center_lay1)*[100]+len(center_lay1)*[0]    def feature_vector(self, pos_x, pos_y):        grid_space = 1000 * np.arange(0.5, 10.5, 1)        centers = list(itertools.product(grid_space, grid_space))        radius_coarse = 750        actual_pos = len(centers) * [(pos_x, pos_y)]        iter = len(centers)        distances = np.array([distance.euclidean(centers[i], actual_pos[i]) for i in range(iter)])        inside_circle = (distances <= radius_coarse)        inside_circle = inside_circle.astype(int)        return inside_circle    def sample_parametrized_actions(self, x_s):        theta_mu_x = np.repeat(0, len(self.centers))        theta_sd_x = np.repeat(2, len(self.centers))        theta_mu_y = np.repeat(0, len(self.centers))        theta_sd_y = np.repeat(2, len(self.centers))        mu_x = np.dot(theta_mu_x, x_s)        sd_x = np.dot(theta_sd_x, x_s)        mu_y = np.dot(theta_mu_y, x_s)        sd_y = np.dot(theta_sd_y, x_s)        sample_vx = norm.rvs(loc=mu_x, scale=sd_x, size=1)[0]        sample_vy = norm.rvs(loc=mu_y, scale=sd_y, size=1)[0]        return (sample_vx, sample_vy)    def estimate_value_function(self,x_s):        # change 200 according to the number of targets        weights = np.repeat(200, len(centers))        v_hat = np.dot(weights, x_s)        return v_hat        def catch_targets(self, starting_x, starting_y, angle, level_action, enviroment):        # print("starting at : " +str((starting_x,starting_y)))        cum_targets_starting = sum([enviroment.targets[i][2] for i in range(self.max_targets)])        cum_threats_starting =  sum([enviroment.threats[i][2] for i in range(self.n_threats)])        for f in range(1, self.freq_sampling + 1):            add_x = starting_x + (f / self.freq_sampling) * self.velocities[level_action] * np.cos(angle)            add_y = starting_y + (f / self.freq_sampling) * self.velocities[level_action] * np.sin(angle)            if add_x > self.L:                add_x = 2 * self.L - add_x            if add_x < 0:                add_x = -1 * add_x            if add_y > self.L:                add_y = 2 * self.L - add_y            if add_y < 0:                add_y = -1 * add_y            # print("sampling at : " + str((add_x, add_y)))            sample_loc = [(add_x, add_y)] * self.max_targets            # print("max targets " +str(self.max_targets))            segments = [(enviroment.targets[i][0],enviroment.targets[i][1]) for i in range(self.max_targets)]            dist_segments = [distance.euclidean(sample_loc[comp], segments[comp]) for comp in range(self.max_targets)]            dist_segments = np.array(dist_segments)            segments_keys = np.arange(0,self.max_targets)            # print("dist: " +str(dist_segments))            segments_keys = np.arange(0,self.max_targets)            for i in segments_keys[dist_segments <= self.radius]:                # print("uno!  " +str(i))                enviroment.update_target_status(i)        # update the threats        sample_loc = [(add_x, add_y)] * self.n_threats        # print("max targets " +str(self.max_targets))        segments = [(enviroment.threats[i][0], enviroment.threats[i][1]) for i in range(self.n_threats)]        dist_segments = [distance.euclidean(sample_loc[comp], segments[comp]) for comp in range(self.n_threats)]        dist_segments = np.array(dist_segments)        segments_keys = np.arange(0, self.n_threats)        # print("dist: " +str(dist_segments))        segments_keys = np.arange(0, self.n_threats)        for i in segments_keys[dist_segments <= self.radius]:            # print("un threat !  " +str(i))            enviroment.update_threat_status(i)        #####        cum_targets_end=sum([enviroment.targets[i][2] for i in range(self.max_targets)])        cum_threats_end=sum([enviroment.threats[i][2] for i in range(self.n_threats)])        collected_targets=cum_targets_end - cum_targets_starting        collected_threats=cum_threats_end - cum_threats_starting        if (collected_targets+collected_threats)==0:            R=-1        else:            R=5*collected_targets-10*collected_threats        return (add_x, add_y,R)    def update_agent_location(self, coordinates):        self.pos_x = coordinates[0]        self.pos_y = coordinates[1]        self.time_step += 1    def action_epsilon_greedy(self,starting_x, starting_y,epsilon):        state_comp_1 = int(np.floor(starting_x / self.w))        state_comp_2 = int(np.floor(starting_y / self.w))        # breaking ties        max_val = max(self.grid[state_comp_1, state_comp_2, :])        candidates_velocities = np.where(self.grid[state_comp_1, state_comp_2, :] == max_val)[0]        # print(" CANDIDATES //////  " + str(candidates_velocities))        weights = [1 / len(candidates_velocities)] * len(candidates_velocities)        prob_action_max = np.ones((len(self.velocities),))        prob_action_greedy = np.ones((len(self.velocities),))        for a in range(len(self.velocities)):            if not a in candidates_velocities:                prob_action_max[a] = 0        prob_action_max = ((1 - epsilon) * prob_action_max) / (np.sum(prob_action_max))        prob_action_greedy=prob_action_greedy/(np.sum(prob_action_greedy))        probabilities=prob_action_max+prob_action_greedy        greedy_action = np.random.choice(candidates_velocities, size=1, p=weights)[0]        if np.random.uniform(0, 1) < epsilon:            # print("Random action//////")            rand_action = np.random.randint(0, len(self.velocities), size=1)[0]            # print("Random action//////  " +str(rand_action))            return (rand_action,probabilities)        else:            return (greedy_action,probabilities)    def temporal_difference_update(self, epsilon, enviroment):        # print("inicio :  " +str((self.pos_x, self.pos_y,self.time_step)))        time_step = self.time_step        angle = 2 * np.pi * np.random.uniform(0, 1)        # print("angle, velocity :  " + str((np.rad2deg(angle),self.velocities[action])))        cum_targets_before = self.cum_targets        action = self.actions[self.time_step]        coordinates = self.catch_targets(angle, action, enviroment)        cum_targets_after = self.cum_targets        reward = cum_targets_after - cum_targets_before        state_comp_1 = int(np.floor(self.pos_x / self.w))        state_comp_2 = int(np.floor(self.pos_y / self.w))        base_val = copy.deepcopy(self.grid[state_comp_1, state_comp_2, self.time_step, action])        self.update_agent_location(coordinates)        state_comp_1_prime = int(np.floor(self.pos_x / self.w))        state_comp_2_prime = int(np.floor(self.pos_y / self.w))        self.actions[self.time_step] = self.next_action_e_greedy(epsilon)        # if reward != 0:        #     print("reward :  " + str((self.pos_x, self.pos_y, self.time_step, reward)))        obj_val = copy.deepcopy(            self.grid[state_comp_1_prime, state_comp_2_prime, self.time_step, self.actions[self.time_step]])        # print("delta :  " + str((base_val- obj_val)))        new_val = base_val + \                  self.alpha * (reward + self.gamma * obj_val - base_val)        self.grid[state_comp_1, state_comp_2, (time_step - 1), self.actions[(time_step - 1)]] = base_val + \                                                                                                self.alpha * (                                                                                                            reward + self.gamma * obj_val - base_val)        # print("update :  " + str(new_val))    def reset_agent(self):        self.pos_x = 0        self.pos_y = 0        self.cum_targets = 0        self.time_step = 0    def episode_filtered(self, path_x, path_y, rewards, action_history):        # pos_x=np.array([1,2,3,1,4,2,5])        # pos_y=np.array([1,2,3,1,4,2,5])        # R=np.array([2,3,1,4,5,1,6])        pos_x = path_x[::-1]        pos_y = path_y[::-1]        R = rewards[::-1]        # Updated with the self.T        T = self.T        G = np.array([0] * T)        cum_R = 0        filter = []        aux_list = [(pos_x[i], pos_y[i],action_history[i]) for i in range(T)]        for i in range(T):            cum_R = self.gamma * cum_R + R[i]            if not aux_list[i] in aux_list[(i + 1):]:                G[i] = cum_R                filter.append(True)            else:                filter.append(False)        pos_x = pos_x[filter]        pos_y = pos_y[filter]        G = G[filter]        actions = action_history[filter]        return (pos_x, pos_y, actions, G)    def generate_episode_b(self,epsilon,enviroment,episode):        path_x = [self.pos_x]        path_y = [self.pos_y]        level_action_b,probabilities = self.action_epsilon_greedy(self.pos_x, self.pos_y, epsilon)        angle = 2 * np.pi * np.random.uniform(0, 1)        new_pos_x, new_pos_y, r = self.catch_targets(self.pos_x, self.pos_y, angle, level_action_b, enviroment)        actions_b = [level_action_b]        b_sa=[probabilities]        rewards_historic = [r]        path_x.append(new_pos_x)        path_y.append(new_pos_y)        for t in range(1, self.T):            # time.sleep(0.1)            level_action_b,probabilities = self.action_epsilon_greedy(path_x[-1], path_y[-1], epsilon)            angle = 2 * np.pi * np.random.uniform(0, 1)            actions_b.append(level_action_b)            b_sa.append(probabilities)            new_pos_x, new_pos_y, r = self.catch_targets(path_x[-1], path_y[-1], angle, level_action_b, enviroment)            rewards_historic.append(r)            path_x.append(new_pos_x)            path_y.append(new_pos_y)        path_x = np.array(path_x[0:(self.T)])        path_y = np.array(path_y[0:(self.T)])        rewards = np.array(rewards_historic)        actions_b = np.array(actions_b)        b_sa=np.array(b_sa)        episode_target_x=np.array([enviroment.targets[i][0] for i in range(self.max_targets)])        episode_target_y=np.array([enviroment.targets[i][1] for i in range(self.max_targets)])        episode_threats_x=np.array([enviroment.targets[i][0] for i in range(self.n_threats)])        episode_threats_y=np.array([enviroment.targets[i][1] for i in range(self.n_threats)])        self.paths_x[episode, :] = path_x        self.paths_y[episode, :] = path_y        self.targets_x[episode, :] = episode_target_x        self.targets_y[episode, :] = episode_target_y        self.threats_x[episode, :] = episode_threats_x        self.threats_x[episode, :] = episode_threats_y        return (path_x, path_y, actions_b,rewards,b_sa)    def update_table_qs(self, path_x, path_y, actions, rewards, b_sa):        # print("states " +str((ind_x,ind_y)))        #        # new_qsa = (self.counts[ind_x, ind_y, actions[i]] * self.grid[ind_x, ind_y, actions[i]] + \        #           discounted_rewards[i])/(self.counts[ind_x, ind_y, actions[i]] +1)        #        # # print("n*q + g  " +str((ind_x,ind_y)))        #        # self.grid[ind_x, ind_y, actions[i]] = new_qsa        # self.counts[ind_x, ind_y, actions[i]] += 1        path_x = path_x[::-1]        path_y = path_y[::-1]        R = rewards[::-1]        actions=actions[::-1]        b_sa = b_sa[::-1]        T_prime = len(path_x)        G=0        W=1        for i in range(T_prime):            # print(" action " + str(actions[i]))            # print(" re " + str(R[i]))            # print(" W " + str(W))            G=self.gamma*G+ R[i]            ind_x = int(np.floor(path_x[i] / self.w))            ind_y = int(np.floor(path_y[i] / self.w))            self.C[ind_x, ind_y, actions[i]] += W            new_qsa= (W/(copy.deepcopy(self.C[ind_x, ind_y, actions[i]])))*\                     (G-copy.deepcopy(self.grid[ind_x, ind_y, actions[i]]))            self.grid[ind_x, ind_y, actions[i]]+=new_qsa            action_pi=np.argmax(self.grid[ind_x, ind_y, :])            if action_pi!=actions[i]:                break            W*=(1/b_sa[i][actions[i]])    def sarsa_montecarlo(self,L, n_cluster,corr, episodes):        epsilon = 0.1        qsa_origin=np.zeros((episodes,))        qsa_center_target_1=np.zeros((episodes,))        for l in range(episodes):            #ojo            enviroment= Enviroment(L, n_cluster, corr,20,20,20)            print("episode: " + str(l))            # time.sleep(0.5)            path_x, path_y, actions_b, rewards, b_sa=self.generate_episode_b(epsilon,enviroment,l)            # print(" path x " + str(path_x))            # print(" path y " + str(path_y))            # print(" R " + str(rewards))            # print(" A " + str(actions_b))            self.update_table_qs( path_x, path_y, actions_b, rewards, b_sa)            qsa_origin[l]=np.max(self.grid[0,0,:])            s_x = int(np.floor(self.n_x*3/4))            s_y = int(np.floor(self.n_y*3/ 4))            qsa_center_target_1[l]=np.max(self.grid[s_x,s_y,:])            self.reset_agent()            # enviroment.reset_target_status()        return (qsa_origin, qsa_center_target_1)    def optimal_policy(self):        for i in range(self.n_x):            for j in range(self.n_y):                self.policy[i,j]=np.argmax(self.grid[i,j,:])        return self.policy    def optimal_q_values(self):        matrix=np.zeros((self.n_x, self.n_y))        for i in range(self.n_x):            for j in range(self.n_y):                matrix[i, j] = np.max(self.grid[i, j, :])        return matrix    def plot_paths(self,enviroment):        X=self.paths_x        Y=self.paths_y        starting_slice=self.episodes-100        end_slice=self.episodes        plt.xlim([0, self.L])        plt.ylim([0, self.L])        plt.plot(X[episodes - 1, :], Y[episodes - 1, :], '--o', color='#65C7F5')        plt.plot(X[episodes - 2, :], Y[episodes - 2, :], '--o', color='#9AF57A')        plt.plot(X[episodes - 3, :], Y[episodes - 3, :], '--o', color='#F5D57A')        plt.plot(X[episodes - 4, :], Y[episodes - 4, :], '--o', color='#E6AEF8')        plt.plot(X[episodes - 5, :], Y[episodes - 5, :], '--o', color='#F8BDAE')        xt = []        yt = []        for i in range(self.max_targets):            xt.append(enviroment.targets[i][0])            yt.append(enviroment.targets[i][1])            plt.scatter(xt, yt, color='red')        plt.show()    def save_paths(self,n_cluster,corr):        X = self.paths_x        Y = self.paths_y        starting_slice = self.episodes - 100        end_slice = self.episodes        path_x = X[starting_slice:end_slice, :]        path_y = Y[starting_slice:end_slice, :]        targets_x=self.targets_x[starting_slice:end_slice, :]        targets_y=self.targets_y[starting_slice:end_slice, :]        threats_x=self.threats_x[starting_slice:end_slice, :]        threats_y=self.threats_y[starting_slice:end_slice, :]        name_1='./paths/L_100_modes_' +str(n_cluster) +'_corr_'+str(corr)+'_path_x.csv'        name_2='./paths/L_100_modes_' +str(n_cluster) +'_corr_'+str(corr)+'_path_y.csv'        name_3='./paths/L_100_modes_' +str(n_cluster) +'_corr_'+str(corr)+'_targets_x.csv'        name_4='./paths/L_100_modes_' +str(n_cluster) +'_corr_'+str(corr)+'_targets_y.csv'        name_5='./paths/L_100_modes_' +str(n_cluster) +'_corr_'+str(corr)+'_threats_x.csv'        name_6='./paths/L_100_modes_' +str(n_cluster) +'_corr_'+str(corr)+'_threats_y.csv'        elements={0:(name_1,path_x),                  1:(name_2,path_y),                  2:(name_3,targets_x),                  3:(name_4,targets_y),                  4:(name_5,threats_x),                  5:(name_6,threats_y)}        for i in range(6):            file_name=elements[i][0]            np.savetxt(file_name, elements[i][1], delimiter=",")    def plot_qsa_learning(self, qsa_origin, qsa_center_target_1):        t=(np.arange(0,self.episodes))        plt.plot(t, qsa_origin, label="qsa origin")        plt.plot(t, qsa_center_target_1, label="qsa target 1")        plt.legend()        plt.show()    def plot_qsa(self):        qs=self.optimal_q_values()        plt.imshow(qs, interpolation='none')        plt.colorbar()        plt.show()    def plot_qsa_all_actions(self):        pi_0=self.optimal_policy(0)        pi_1=self.optimal_policy(1)        pi_2=self.optimal_policy(2)        plt.figure()        # leftmost        plt.subplot(1, 3, 1)        plt.imshow(pi_0, cmap="Blues_r")        plt.title('SubPlot Example')        # middle        plt.subplot(1, 3, 2)        plt.imshow(pi_1, cmap="Blues_r")        plt.xlabel('time (s)')        plt.ylabel('Undamped')        # rightmost figure        plt.subplot(1, 3, 3)        plt.imshow(pi_2, cmap="Blues_r")        plt.xlabel('time (s)')        plt.ylabel('Undamped')        plt.show()        # plt.subplot(2, 1, 1)        # plt.imshow(pi_0, cmap="Blues_r")        # plt.subplot(2, 1, 2)        # plt.imshow(pi_1, cmap="Blues_r")        # plt.subplot(2, 1, 3)        # plt.imshow(pi_2, cmap="Blues_r")        # plt.xlabel("Day")        # plt.ylabel("Rain")        # plt.suptitle("2009 weather")        # plt.colorbar()        # plt.show()        # pi_0=self.optimal_policy(0)        # pi_1=self.optimal_policy(1)        # pi_2=self.optimal_policy(2)        #        # plt.rcParams["figure.figsize"] = [7.00, 3.50]        # plt.rcParams["figure.autolayout"] = True        #        # plt.subplot(1, 4, 1)        # plt.imshow(pi_0, cmap="Blues_r")        # plt.subplot(1, 4, 2)        # plt.imshow(pi_1, cmap="Blues_r")        # plt.subplot(1, 4, 3)        # plt.imshow(pi_2, cmap="Blues_r")        # plt.show()        # pi=self.optimal_policy(ind_action)        # plt.imshow(pi, interpolation='none')        # plt.colorbar()        # plt.show()    def plot_policy(self):        pi=self.optimal_policy()        plt.imshow(pi, interpolation='none')        plt.colorbar()        # plt.show()        plt.savefig('1_modes.pdf')w_cell=5L=100T=50max_targets=60freq_sampling=2radius=5velocities=[5,15,40]episodes=300n_cluster=3corr=0a1=Agent(w_cell, L, T, velocities, max_targets, freq_sampling,             radius, episodes)origen,zone_1=a1.sarsa_montecarlo(L,n_cluster,corr,episodes)# pi=a1.optimal_policy()## qs=a1.optimal_q_values()# a1.plot_qsa_learning(origen, zone_1)a1.save_paths(n_cluster,corr)a1.plot_policy()a1.plot_paths(t1)